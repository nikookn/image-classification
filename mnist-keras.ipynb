{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE MNIST DATABASE of handwritten digits\n",
    "\n",
    "The MNIST database of handwritten digits, available from [here](http://yann.lecun.com/exdb/mnist/), has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n",
    "\n",
    "Four files are available on this site:\n",
    "```\n",
    "train-images-idx3-ubyte.gz:  training set images (9912422 bytes) \n",
    "train-labels-idx1-ubyte.gz:  training set labels (28881 bytes) \n",
    "t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes) \n",
    "t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)\n",
    "```\n",
    "These files are not in any standard image format. You have to write your own (very simple) program to read them. However, [keras](https://www.tensorflow.org/api_docs/python/tf/keras) has helper function, [``keras.datasets.mnist.load_data``](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data), that allows you to load this dataset easily. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and prepare the MNIST dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``x_train`` and ``x_test`` are ``uint8`` arrays of grayscale image data with shapes ``(num_samples, 28, 28)``. ``y_train`` and ``y_test`` are ``uint8`` arrays of digit labels (integers in range 0-9) with shapes ``(num_samples,)``.\n",
    "\n",
    "Let's normalize the image samples from integers (in range 0-255) to floating-point numbers (in range 0.0-1.0):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build the ``Sequential`` model by stacking layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x145723490>\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each example the model returns a vector of \"[logits](https://developers.google.com/machine-learning/glossary#logits)\" scores, one for each class.\n",
    "\n",
    "Now, we choose an optimizer and loss function for training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [``losses.SparseCategoricalCrossentropy``](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) loss takes a vector of logits and a ``True`` index and returns a scalar loss for each example. This loss is equal to the negative log probability of the true class: It is zero if the model is sure of the correct class.\n",
    "\n",
    "Now, let's train our model. The ``fit`` method adjusts the model parameters to minimize the loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 944us/step - loss: 0.2955 - accuracy: 0.9142\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 857us/step - loss: 0.1415 - accuracy: 0.9589\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 803us/step - loss: 0.1060 - accuracy: 0.9682\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 1s 647us/step - loss: 0.0874 - accuracy: 0.9728\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 1s 642us/step - loss: 0.0736 - accuracy: 0.9767\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 1s 713us/step - loss: 0.0627 - accuracy: 0.9799\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 1s 686us/step - loss: 0.0575 - accuracy: 0.9811\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 1s 705us/step - loss: 0.0523 - accuracy: 0.9827\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 1s 676us/step - loss: 0.0459 - accuracy: 0.9851\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 1s 685us/step - loss: 0.0436 - accuracy: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1458686d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that our model is trained, it's time to evaluate its performance on an unseen dataset. The ``evaluate`` method checks the models performance on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0661 - accuracy: 0.9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06610903143882751, 0.9811000227928162]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the image classifier is now trained to ~98% accuracy on this dataset.\n",
    "\n",
    "If you want your model to return a probability instead of logits, you can wrap the trained model, and attach the softmax to it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = keras.Sequential([\n",
    "  model,\n",
    "  keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_model(x_test[:1]).numpy().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Now, let's change our model to take advantage of convolutional neural networks. But, we need to add a channels dimension before feeding the data to [``keras.layers.Conv2D``](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build the ``Sequential`` model by stacking layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x145fa0970>\n"
     ]
    }
   ],
   "source": [
    "conv_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "print(conv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we choose an optimizer and loss function for training for the new model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.compile(optimizer='adam', \n",
    "                   loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our new model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.1371 - accuracy: 0.9585\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 35s 18ms/step - loss: 0.0424 - accuracy: 0.9864\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.0233 - accuracy: 0.9921\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0132 - accuracy: 0.9955\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0109 - accuracy: 0.9963\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0061 - accuracy: 0.9980\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0061 - accuracy: 0.9981\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0052 - accuracy: 0.9983\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0040 - accuracy: 0.9987\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0038 - accuracy: 0.9987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x145fc9eb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that our model is trained, let's evaluate its performance on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0842 - accuracy: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08415018022060394, 0.9825000166893005]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the image classifier is now trained to ~98% accuracy on this dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
