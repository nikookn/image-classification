{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE MNIST DATABASE of handwritten digits\n",
    "\n",
    "The MNIST database of handwritten digits, available from [here](http://yann.lecun.com/exdb/mnist/), has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n",
    "\n",
    "Four files are available on this site:\n",
    "```\n",
    "train-images-idx3-ubyte.gz:  training set images\n",
    "train-labels-idx1-ubyte.gz:  training set labels\n",
    "t10k-images-idx3-ubyte.gz:   test set images\n",
    "t10k-labels-idx1-ubyte.gz:   test set labels\n",
    "```\n",
    "These files are not in any standard image format. You have to write your own (very simple) program to read them. So, let's download, load and prepare the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "data_path = Path(\"data\") / \"mnist\" / \"raw\"\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mnist_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "train_images = \"train-images-idx3-ubyte.gz\"\n",
    "train_labels = \"train-labels-idx1-ubyte.gz\"\n",
    "test_images = \"t10k-images-idx3-ubyte.gz\"\n",
    "test_labels = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "for filename in [train_images, train_labels, test_images, test_labels]:\n",
    "    if not (data_path / filename).exists():\n",
    "        content = requests.get(mnist_url + filename).content\n",
    "        (data_path / filename).open(\"wb\").write(content)\n",
    "\n",
    "with gzip.open((data_path / train_images).as_posix(), \"r\") as f:\n",
    "    x_train = np.frombuffer(f.read(), dtype=np.uint8, offset=16).reshape((-1, 28, 28))\n",
    "with gzip.open((data_path / train_labels).as_posix(), \"r\") as f:\n",
    "    y_train = np.frombuffer(f.read(), dtype=np.uint8, offset=8)\n",
    "with gzip.open((data_path / test_images).as_posix(), \"r\") as f:\n",
    "    x_test = np.frombuffer(f.read(), dtype=np.uint8, offset=16).reshape((-1, 28, 28))\n",
    "with gzip.open((data_path / test_labels).as_posix(), \"r\") as f:\n",
    "    y_test = np.frombuffer(f.read(), dtype=np.uint8, offset=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``x_train`` and ``x_test`` are ``uint8`` arrays of grayscale image data with shapes ``(num_samples, 28, 28)``. ``y_train`` and ``y_test`` are ``uint8`` arrays of digit labels (integers in range 0-9) with shapes ``(num_samples,)``.\n",
    "\n",
    "Let's normalize the image samples from integers (in range 0-255) to floating-point numbers (in range 0.0-1.0):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to convert our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-82e3ea382fa4>:2: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  y_train = torch.tensor(y_train, dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch’s [TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) is a Dataset wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train.\n",
    "\n",
    "Pytorch’s [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) is responsible for managing batches. You can create a ``DataLoader`` from any ``Dataset``. ``DataLoader`` makes it easier to iterate over batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "test_set = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, you should know that there is an easier way to download, load and prepare  MNIST dataset and create a ``DataLoader`` from the loaded dataset.\n",
    "\n",
    "```python\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True, num_workers=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build the ``Sequential`` model by stacking layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Lambda()\n",
      "  (1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.2, inplace=False)\n",
      "  (4): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we choose an optimizer and loss function for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [``nn.CrossEntropyLoss``](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) loss combines [``nn.LogSoftmax()``](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax) and [``nn.NLLLoss()``](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) in one single class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outs, labels):\n",
    "    outputs = np.argmax(outs, axis=1)\n",
    "    return np.sum(outputs==labels) / float(labels.size)\n",
    "metrics = {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, loss_fn, train_dl, metrics, epoches=1):\n",
    "    mtr_sum = []\n",
    "    model.train()\n",
    "    for epoch in range(epoches):\n",
    "        for x, y in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            summary_batch = {metric: metrics[metric](preds.detach().numpy(), y.detach().numpy()) for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item()\n",
    "            mtr_sum.append(summary_batch)\n",
    "        metrics_mean = {metric: np.mean([mtr[metric] for mtr in mtr_sum]) for metric in mtr_sum[0]}\n",
    "        metrics_string = \" ; \".join(\"{}: {:09.6f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "        print(f\"Epoch {epoch+1} - \", metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -  accuracy: 00.909300 ; loss: 00.321321\n",
      "Epoch 2 -  accuracy: 00.931658 ; loss: 00.236484\n",
      "Epoch 3 -  accuracy: 00.942900 ; loss: 00.194619\n",
      "Epoch 4 -  accuracy: 00.950079 ; loss: 00.168788\n",
      "Epoch 5 -  accuracy: 00.955283 ; loss: 00.150335\n",
      "Epoch 6 -  accuracy: 00.959075 ; loss: 00.136530\n",
      "Epoch 7 -  accuracy: 00.961964 ; loss: 00.126071\n",
      "Epoch 8 -  accuracy: 00.964506 ; loss: 00.117018\n",
      "Epoch 9 -  accuracy: 00.966650 ; loss: 00.109446\n",
      "Epoch 10 -  accuracy: 00.968472 ; loss: 00.103121\n"
     ]
    }
   ],
   "source": [
    "fit(model, optimizer, loss_fn, train_loader, metrics, epoches=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that our model is trained, it's time to evaluate its performance on an unseen dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, test_dl, metrics):\n",
    "    mtr_sum = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dl:\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, y)\n",
    "            summary_batch = {metric: metrics[metric](preds.detach().numpy(), y.detach().numpy()) for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item()\n",
    "            mtr_sum.append(summary_batch)\n",
    "        metrics_mean = {metric: np.mean([mtr[metric] for mtr in mtr_sum]) for metric in mtr_sum[0]}\n",
    "        metrics_string = \" ; \".join(\"{}: {:09.6f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "        print(metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 00.980431 ; loss: 00.071889\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, loss_fn, test_loader, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the image classifier is now trained to ~98% accuracy on this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Now, let's change our model to take advantage of convolutional neural networks. We will use Pytorch’s predefined [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) class as our convolutional layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 26 * 26, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we choose an optimizer and loss function for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_loss_fn = nn.CrossEntropyLoss()\n",
    "conv_optimizer = optim.Adam(conv_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to resize the images to have their number of channels, ``1``, before their hight and width.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.view(-1, 1, x_train.size(1), x_train.size(2))\n",
    "x_test = x_test.view(-1, 1, x_test.size(1), x_test.size(2))\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "test_set = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use the same ``fit`` function that we developed above to train our new model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -  accuracy: 00.947083 ; loss: 00.178761\n",
      "Epoch 2 -  accuracy: 00.964367 ; loss: 00.118494\n",
      "Epoch 3 -  accuracy: 00.972544 ; loss: 00.090331\n",
      "Epoch 4 -  accuracy: 00.977596 ; loss: 00.073194\n",
      "Epoch 5 -  accuracy: 00.981197 ; loss: 00.061295\n",
      "Epoch 6 -  accuracy: 00.983819 ; loss: 00.052630\n",
      "Epoch 7 -  accuracy: 00.985774 ; loss: 00.046186\n",
      "Epoch 8 -  accuracy: 00.987300 ; loss: 00.041166\n",
      "Epoch 9 -  accuracy: 00.988456 ; loss: 00.037310\n",
      "Epoch 10 -  accuracy: 00.989485 ; loss: 00.034013\n"
     ]
    }
   ],
   "source": [
    "fit(conv_model, conv_optimizer, conv_loss_fn, train_loader, metrics, epoches=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we'll use the same ``evaluate`` function that we developed above to test our model on an unseen dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 00.981130 ; loss: 00.106488\n"
     ]
    }
   ],
   "source": [
    "evaluate(conv_model, conv_loss_fn, test_loader, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the image classifier is now trained to ~98% accuracy on this dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
